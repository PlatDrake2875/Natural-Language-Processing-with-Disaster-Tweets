{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id keyword location                                               text  \\\n",
      "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
      "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
      "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
      "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
      "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
      "\n",
      "   target  \n",
      "0       1  \n",
      "1       1  \n",
      "2       1  \n",
      "3       1  \n",
      "4       1  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "df = pd.read_csv('data/train.csv')\n",
    "print(df.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_length = len(df)\n",
    "\n",
    "cnt_disasters = len(df[df['target'] == 1])\n",
    "cnt_not_disasters = df_length - cnt_disasters\n",
    "\n",
    "print(f\"Number of events: {df_length}\")\n",
    "print(f\"Number of disasters: {cnt_disasters}\")\n",
    "print(f\"Number of false disasters: {cnt_not_disasters}\")\n",
    "print()\n",
    "\n",
    "prob_disasters = cnt_disasters / df_length\n",
    "print(f\"Percenrage of disasters: {100 * prob_disasters:0.2f}%\")\n",
    "print(f\"Percentage of false disasters: {100 * (1 - prob_disasters):0.2f}%\")\n",
    "print()\n",
    "\n",
    "condition_disaster_location = (df['target'] == 1) & (df['location'].isna() == False)\n",
    "cnt_disasters_location = len(df[condition_disaster_location]) / cnt_disasters\n",
    "print(f\"Percentage of disasters with location provided: {100 * cnt_disasters_location:0.2f}%\")\n",
    "print(f\"Percentage of disasters without location provided: {100 * (1 - cnt_disasters_location):0.2f}%\")\n",
    "print()\n",
    "\n",
    "condition_disaster_keyword = (df['target'] == 1) & (df['keyword'].isna() == False)\n",
    "cnt_disasters_keyword = len(df[condition_disaster_keyword]) / cnt_disasters\n",
    "print(f\"Percentage of disasters with location provided: {100 * cnt_disasters_keyword:0.2f}%\")\n",
    "print(f\"Percentage of disasters without location provided: {100 * (1 - cnt_disasters_keyword):0.2f}%\")\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    " \n",
    "nltk.download('stopwords')\n",
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords \n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def remove_links(text):\n",
    "    pattern = r\"https?://\\S+\"\n",
    "    text = re.sub(pattern, '', text)\n",
    "    return text\n",
    "\n",
    "def remove_numbers(text):\n",
    "    pattern = r\"\\d+\"\n",
    "    text = re.sub(pattern, \"\", text)\n",
    "    return text\n",
    "\n",
    "def preprocess(text: str):\n",
    "    text = text.lower()\n",
    "    text = remove_links(text) \n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))  # Remove punctuation\n",
    "    text = remove_numbers(text)\n",
    "    text = \" \".join([w for w in text.split() if w not in stop_words])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_complete = df[['keyword', 'location', 'text']].fillna('').agg(' '.join, axis=1).str.strip()\n",
    "print(dataset_complete.head(5))\n",
    "\n",
    "dataset = dataset_complete.apply(preprocess)\n",
    "dataset = dataset.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    Our Deeds are the Reason of this #earthquake M...\n",
      "1               Forest fire near La Ronge Sask. Canada\n",
      "2    All residents asked to 'shelter in place' are ...\n",
      "3    13,000 people receive #wildfires evacuation or...\n",
      "4    Just got sent this photo from Ruby #Alaska as ...\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "dataset_clear = df[['text']].agg(' '.join, axis=1)\n",
    "print(dataset_clear.head(5))\n",
    "\n",
    "dataset = dataset_clear.apply(preprocess)\n",
    "dataset = dataset.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_complete = df[['keyword', 'text']].fillna('').agg(' '.join, axis=1).str.strip()\n",
    "print(dataset_complete.head(5))\n",
    "\n",
    "dataset = dataset_complete.apply(preprocess)\n",
    "dataset = dataset.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_complete = df[['location', 'text']].fillna('').agg(' '.join, axis=1).str.strip()\n",
    "print(dataset_complete.head(5))\n",
    "\n",
    "dataset = dataset_complete.apply(preprocess)\n",
    "dataset = dataset.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TFIDF + DecisionTree\n",
    "\n",
    "# F1 Score = F1 Score * 100\n",
    "# Keyword + location + text -> F1 Score 66.82\n",
    "# Keyword + text            -> F1 Score 68.39\n",
    "# Location + text           -> F1 Score 68.27\n",
    "# Text                      -> F1 Score 69.02\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "X = vectorizer.fit_transform(dataset)\n",
    "y = df['target'].tolist()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "\n",
    "clf = DecisionTreeClassifier(random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "accuracy = f1_score(y_test, y_pred)\n",
    "print(f\"F1 Score: {100 * accuracy:0.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HashVectorizer + DecisionTree\n",
    "# F1Score = F1Score * 100\n",
    "# Text + n_features=32-> F1 Score - 52.94\n",
    "# Text + n_features=4196 -> F1 Score - 66.57\n",
    "\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "vectorizer = HashingVectorizer(n_features=4196)\n",
    "X = vectorizer.transform(dataset)\n",
    "y = df['target'].tolist()\n",
    "    \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "\n",
    "clf = DecisionTreeClassifier(random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "accuracy = f1_score(y_test, y_pred)\n",
    "print(f\"F1 Score: {100 * accuracy:0.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word2Vec\n",
    "# Text -> 53.20\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "\n",
    "tokenized_docs = [doc.split() for doc in dataset]\n",
    "w2v_model = Word2Vec(tokenized_docs, vector_size=64, window=5, min_count=1, workers=16)\n",
    "\n",
    "def document_vector(doc):\n",
    "    tokens = doc.split()\n",
    "    vectors = [w2v_model.wv[token] for token in tokens if token in w2v_model.wv]\n",
    "    return np.mean(vectors, axis=0) if vectors else np.zeros(w2v_model.vector_size)\n",
    "\n",
    "doc_vectors = [document_vector(doc) for doc in dataset]\n",
    "X = np.array(doc_vectors)\n",
    "y = df['target'].tolist()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "clf = DecisionTreeClassifier(random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "accuracy = f1_score(y_test, y_pred)\n",
    "print(f\"F1 Score: {100 * accuracy:0.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0         deeds reason earthquake may allah forgive us\n",
      "1                forest fire near la ronge sask canada\n",
      "2    residents asked shelter place notified officer...\n",
      "3    people receive wildfires evacuation orders cal...\n",
      "4    got sent photo ruby alaska smoke wildfires pou...\n",
      "dtype: object\n",
      "(1523, 16850)\n",
      "F1 Score: 73.141\n"
     ]
    }
   ],
   "source": [
    "dataset_clear = df[['text']].agg(' '.join, axis=1)\n",
    "\n",
    "dataset = dataset_clear.apply(preprocess)\n",
    "print(dataset.head(5))\n",
    "dataset = dataset.values.tolist()\n",
    "\n",
    "# TFIDF + Logistic Regression\n",
    "\n",
    "# F1 Score = F1 Score * 100\n",
    "# Keyword + location + text -> F1 Score 66.82\n",
    "# Keyword + text            -> F1 Score 68.39\n",
    "# Location + text           -> F1 Score 68.27\n",
    "# Text                      -> F1 Score 73.14\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "X = vectorizer.fit_transform(dataset)\n",
    "y = df['target'].tolist()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "\n",
    "clf = LogisticRegression(random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "print(X_test.shape)\n",
    "y_pred = clf.predict(X_test)\n",
    "accuracy = f1_score(y_test, y_pred)\n",
    "print(f\"F1 Score: {100 * accuracy:0.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                          happened terrible car crash\n",
      "1    heard earthquake different cities stay safe ev...\n",
      "2    forest fire spot pond geese fleeing across str...\n",
      "3                apocalypse lighting spokane wildfires\n",
      "4                  typhoon soudelor kills china taiwan\n",
      "dtype: object\n",
      "(3263, 16850)\n"
     ]
    }
   ],
   "source": [
    "# Test sampling\n",
    "df_test = pd.read_csv('data/test.csv')\n",
    "dataset_clear = df_test[['text']].agg(' '.join, axis=1)\n",
    "\n",
    "dataset = dataset_clear.apply(preprocess)\n",
    "print(dataset.head(5))\n",
    "dataset = dataset.values.tolist()\n",
    "\n",
    "X_test = vectorizer.transform(dataset)\n",
    "\n",
    "print(X_test.shape)\n",
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df_test.drop(['keyword', 'location', 'text'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  target\n",
       "0   0       1\n",
       "1   2       1\n",
       "2   3       1\n",
       "3   9       0\n",
       "4  11       1\n",
       "5  12       1\n",
       "6  21       0\n",
       "7  22       0\n",
       "8  27       0\n",
       "9  29       0"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test['target'] = y_pred\n",
    "df_test[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
